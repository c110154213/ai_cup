## 目錄

- [分割](#分割)
- [合併](#合併)
- [資料篩選](#資料篩選)
- [模型訓練](#模型訓練)
- [解碼](#解碼)
- [後處理](#後處理)

### 分割
(將文本切成合適的大小)  
此部分需引入文本 訓練集或驗證集

分割:

將文本以文字字典順序做排序，再以分割條件為  
			a.換行符號(\n)  
			b.點空白(\. )(前為Dr時不匹配)(括弧內不匹配)(後為大寫字母不匹配)  
   最後輸出格式為
   ```sh
			File1		start1	content1  
			File1	 	start2	content2  
			…  
			File2		start1	content1  
			File2		start2	content2  
			…  
			FileN		start1	content1  
			FileN		start2	content2  
   ```
中間以Tab(\t)隔開  
File 為文件名  
start為content的起始位置  
content 為已分割完的文本內容  
輸出為split.tsv

### 合併  
(製造出訓練集)  
此部分需引入分割檔(split.tsv)與正解(answer.txt)  
合併:  
		將answer.txt中PHI存在的填入split.tsv中，若無，則填	入PHI: NULL  
    	        同一行中若有大於一個PHI時，以文字\n作為區隔，  
	        例:  DOCTOR: A\n DOCTOR: B  
		若有正規化資訊時，以=>接在PHI的後面  
		例: DATE: 19/7/03=>2003-07-19  
			最後輸出格式為:  
   ```
			File1		start1	content1		PHI1
			File1	 	start2	content2                PHI2
			…
			File2		start1	content1		PHI1
			File2		start2	content2		PHI2
			…
			FileN		start1	content1		PHI1
			FileN		start2	content2		PHI2
```
中間以Tab(\t)隔開  
輸出為未篩選的訓練集  

### 資料篩選 
(若訓練資料不需要NULL項)  
移除PHI項為"PHI: NULL"的行

### 模型訓練
此部分需載入訓練集
1. 安裝必要套件:
   使用 !pip install transformers 和 !pip install datasets 安裝 transformers和datasets 庫。  
2. 定義預訓練模型 (PLM) 和special token:  
   選擇預訓練模型 (plm="EleutherAI/pythia-160m")、定義special token，含有序列開始 (bos)、序列結束 (eos)、填充 (pad) 和分隔符號 (sep)。  
3. 進行分詞:  
   從預訓練模型中載入分詞器，並新增特殊標記。  
   設定填充位置為 'left'。  
4. 使用load_dataset載入訓練集資料，拆分為'fid', 'idx', 'content', 'label'  
5. 資料集分割:  
   使用 torch.utils.data.random_split 將訓練資料集分成訓練集和驗證集。  
   這裡使用完全訓練，因此驗證集數量=0。  
6. 定義PAD:  
   PAD_IDX 的值是 tokenizer.pad_token 對應的索引。  
   IGNORED_PAD_IDX= -100，使用-100作為填充標記。  
7. 進行分詞索引和創建DataLoader:  
   7.1將標記轉換為索引並建立自定義匯總函數:
     
	 7.1.1將文本轉換為模型可接受的格式:  
   ```
 	<|endoftext|> data['content']  
  
	 ####  
  
	data['label'] <|END|>
   ```
   
   	 7.1.2 使用 tokenizer 將處理後的文本轉換為模型的輸入，包括索引（input_ids）和注意力遮罩（attention_mask）。  
 	 7.1.3將模型的輸出標籤（input_ids）設置為 IGNORED_PAD_IDX  以忽略填充的影響。
   
   7.2為訓練資料創建 DataLoader。  
9. 批次取樣器:  
   BATCH_SIZE設定為5  
10. 模型配置:  
   使用 AutoConfig 配置模型，並載入預訓練的語言模型。  
11. 使用GPU   
12. 優化器和學習率調度器:  
    設置 AdamW 優化器、線性學習率調度器。  
13. 訓練迴圈:  
    運行訓練迴圈，訓練指定次數。(10次)(lr=7e-5)  
    更新學習率調度器，計算損失。  
14. 保存模型:  
    在每一個迴圈都儲存一個模型與分詞器  
  
請在此區保存模型以及分詞器  
### 解碼

此部分需引入以訓練好的模型以及分詞器與以及分割好的驗證集

輸出為output.txt


### 後處理
(處理簡單錯誤)  
此部分需引入已解碼完成的文本(output.txt)

5.1刪除content為空值的行  
5.2刪除label不符合PHI類別的行  
5.3刪除content字數大於100的行(大於100通常為重複的文字)  
5.4當label順序為”CITY”，”STATE”，”ZIP”時，調整STATE與ZIP的起始位置與結束位置(本模型輸出時，STATE/ZIP 起始位置有錯誤)  
5.5刪除重複或位置錯誤的行(演算方法為，在同樣的file name之下，第i行的起始位置<=第i-1行的結束位置)  

輸出為answer.txt  
